{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfyCZ7l1djIS",
        "outputId": "b9f8c7fb-7068-408a-dac8-bdb805a209bc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training Random Forest Model... \n",
            "Model Training Complete!\n",
            "Model Accuracy: 0.91\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         Anger       1.00      0.00      0.00        44\n",
            "       Content       1.00      0.00      0.00        30\n",
            "Disappointment       1.00      0.00      0.00        12\n",
            "           Joy       0.91      1.00      0.95       897\n",
            "\n",
            "      accuracy                           0.91       983\n",
            "     macro avg       0.98      0.25      0.24       983\n",
            "  weighted avg       0.92      0.91      0.87       983\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import joblib\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "file_path = \"/content/sample_data/amazon_review.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df = df.dropna(subset=['reviewText'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return \"emptyreview\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return \" \".join(tokens) if tokens else \"emptyreview\"\n",
        "\n",
        "df['cleaned_review'] = df['reviewText'].apply(preprocess_text)\n",
        "\n",
        "def map_emotion(rating):\n",
        "    if rating >= 4:\n",
        "        return \"Joy\"\n",
        "    elif rating == 3:\n",
        "        return \"Content\"\n",
        "    elif rating == 2:\n",
        "        return \"Disappointment\"\n",
        "    else:\n",
        "        return \"Anger\"\n",
        "\n",
        "df['emotion'] = df['overall'].apply(map_emotion)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['emotion_encoded'] = label_encoder.fit_transform(df['emotion'])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_review'], df['emotion_encoded'], test_size=0.2, random_state=42)\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=15000, ngram_range=(1,2))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
        "\n",
        "# Apply SMOTE for class balancing\n",
        "smote_strategy = {label: int(count * 1.5) for label, count in Counter(y_train).items()}\n",
        "smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf_dense, y_train)\n",
        "\n",
        "# Train Random Forest Model\n",
        "model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight=\"balanced\")\n",
        "print(\"ðŸš€ Training Random Forest Model... \")\n",
        "model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "\n",
        "joblib.dump(model, \"random_forest_emotion_model.pkl\")\n",
        "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
        "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
        "\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
        "y_test_labels = label_encoder.inverse_transform(y_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
        "print(f\"Model Training Complete!\")\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_labels, y_pred_labels, zero_division=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "muwG3urWeCO7"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "tfidf = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
        "model = joblib.load(\"random_forest_emotion_model.pkl\")\n",
        "\n",
        "# Emotion classification function\n",
        "def analyze_emotions(text):\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    text_tfidf = tfidf.transform([cleaned_text])\n",
        "\n",
        "    predicted_label = model.predict(text_tfidf)[0]\n",
        "    emotion = label_encoder.inverse_transform([predicted_label])[0]\n",
        "    confidence = max(model.predict_proba(text_tfidf)[0])\n",
        "\n",
        "    intensity = round(min(confidence * 1.2, 1.0), 2)\n",
        "    activation = \"High\" if intensity > 0.7 else \"Medium\" if intensity > 0.4 else \"Low\"\n",
        "\n",
        "    secondary_emotion = \"Disappointment\" if emotion == \"Joy\" else \"Joy\"\n",
        "    secondary_intensity = round(max(0, intensity - 0.5), 2)\n",
        "\n",
        "    return {\n",
        "        \"primary\": {\"emotion\": emotion, \"activation\": activation, \"intensity\": intensity},\n",
        "        \"secondary\": {\"emotion\": secondary_emotion, \"activation\": \"Low\", \"intensity\": secondary_intensity}\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langdetect\n",
        "!pip install --quiet deep-translator\n",
        "\n"
      ],
      "metadata": {
        "id": "K1NAID_f1Oje"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "industry_topics = {\n",
        "    \"E-commerce\": {\n",
        "        \"Delivery\": [\"fast delivery\", \"shipping\", \"delivered\", \"on time\", \"slow delivery\"],\n",
        "        \"Quality\": [\"quality\", \"amazing\", \"bad\", \"poor\", \"excellent\", \"terrible\", \"worst\"],\n",
        "        \"Customer Service\": [\"support\", \"rude\", \"helpful\", \"unresponsive\"]\n",
        "    },\n",
        "    \"Healthcare\": {\n",
        "        \"Service\": [\"appointment\", \"wait time\", \"doctor\", \"staff\", \"nurse\"],\n",
        "        \"Facilities\": [\"clean\", \"equipment\", \"hygiene\", \"sanitation\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"industry_topics.json\", \"w\") as file:\n",
        "    json.dump(industry_topics, file, indent=4)\n",
        "\n",
        "print(\"industry_topics.json has been created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b15VoWLd1-Y4",
        "outputId": "18c75fc1-a138-4a0a-d7ae-2241aca27e36",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "industry_topics.json has been created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from langdetect import detect\n",
        "from deep_translator import GoogleTranslator\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "\n",
        "model = joblib.load(\"random_forest_emotion_model.pkl\")\n",
        "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
        "tfidf = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "\n",
        "with open(\"industry_topics.json\", \"r\") as file:\n",
        "    industry_topics = json.load(file)\n",
        "\n",
        "def detect_and_translate(text):\n",
        "    detected_lang = detect(text)\n",
        "    if detected_lang != \"en\":\n",
        "        print(f\"Translating from {detected_lang} to English...\")\n",
        "        return GoogleTranslator(source=detected_lang, target=\"en\").translate(text)\n",
        "    return text\n",
        "\n",
        "def scale_intensity(confidence):\n",
        "    return 0.8 if confidence >= 0.85 else (0.7 if confidence >= 0.6 else round(confidence * 0.8, 2))\n",
        "\n",
        "# Emotion Classification\n",
        "def analyze_emotions(text):\n",
        "    text = detect_and_translate(text)\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    text_tfidf = tfidf.transform([cleaned_text])\n",
        "\n",
        "    proba = model.predict_proba(text_tfidf)[0]\n",
        "    sorted_probs = np.argsort(proba)[::-1]\n",
        "\n",
        "    primary_emotion = label_encoder.inverse_transform([sorted_probs[0]])[0]\n",
        "    confidence = proba[sorted_probs[0]]\n",
        "\n",
        "    print(\"Probabilities:\", dict(zip(label_encoder.classes_, proba)))\n",
        "    print(\"Primary Emotion (Before Thresholding):\", primary_emotion, confidence)\n",
        "\n",
        "    negative_emotions = {\"Disappointment\", \"Anger\", \"Sadness\"}\n",
        "    positive_emotions = {\"Joy\", \"Content\"}\n",
        "\n",
        "    anger_triggers = {\"worst\", \"terrible\", \"awful\", \"horrible\", \"scam\", \"frustrated\", \"rude\", \"never again\"}\n",
        "\n",
        "    if confidence < 0.5:\n",
        "        primary_emotion = \"Neutral\"\n",
        "    elif primary_emotion in positive_emotions and \"poor\" in text.lower():\n",
        "        primary_emotion = \"Disappointment\"\n",
        "    elif any(word in text.lower() for word in anger_triggers):\n",
        "        primary_emotion = \"Anger\"\n",
        "\n",
        "    intensity = scale_intensity(confidence)\n",
        "    activation = \"High\" if intensity > 0.8 else \"Medium\" if intensity > 0.4 else \"Low\"\n",
        "\n",
        "    secondary_emotion = label_encoder.inverse_transform([sorted_probs[1]])[0] if len(sorted_probs) > 1 else \"Neutral\"\n",
        "    secondary_intensity = round(intensity * 0.375, 2)\n",
        "\n",
        "    return {\n",
        "        \"primary\": {\"emotion\": primary_emotion, \"activation\": activation, \"intensity\": intensity},\n",
        "        \"secondary\": {\"emotion\": secondary_emotion, \"activation\": \"Low\", \"intensity\": secondary_intensity},\n",
        "        \"confidence_scores\": dict(zip(label_encoder.classes_, proba))\n",
        "    }\n",
        "\n",
        "# Topic Detection\n",
        "def analyze_topics(text, industry=\"ecommerce\"):\n",
        "    topic_keywords = {\n",
        "        \"ecommerce\": {\n",
        "            \"Delivery\": [\"fast delivery\", \"shipping\", \"delivered\", \"on time\", \"slow delivery\"],\n",
        "            \"Quality\": [\"quality\", \"amazing\", \"bad\", \"poor\", \"excellent\", \"terrible\", \"worst\"],\n",
        "            \"Clothes\": [\"fit\", \"size\", \"clothing\", \"small\", \"tight\", \"loose\", \"perfect size\"]\n",
        "        },\n",
        "        \"healthcare\": {\n",
        "            \"Service\": [\"doctor\", \"nurse\", \"staff\", \"appointment\", \"treatment\"],\n",
        "            \"Facility\": [\"hospital\", \"clean\", \"sanitary\", \"equipment\", \"waiting time\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    detected_topics = {\"main\": [], \"subtopics\": {}}\n",
        "\n",
        "    industry_topics = topic_keywords.get(industry, topic_keywords[\"ecommerce\"])\n",
        "    words = text.lower().split()\n",
        "\n",
        "    for topic, keywords in industry_topics.items():\n",
        "        matched_keywords = [kw for kw in keywords if any(word in kw for word in words)]\n",
        "        if matched_keywords:\n",
        "            detected_topics[\"main\"].append(topic)\n",
        "            detected_topics[\"subtopics\"][topic] = matched_keywords\n",
        "\n",
        "    return detected_topics\n",
        "\n",
        "def compute_adorescore(emotions, topics):\n",
        "    base_scores = {\"Joy\": 90, \"Content\": 70, \"Disappointment\": 30, \"Anger\": -80}\n",
        "\n",
        "    primary_emotion = emotions[\"primary\"][\"emotion\"]\n",
        "    secondary_emotion = emotions[\"secondary\"][\"emotion\"]\n",
        "\n",
        "    primary_score = base_scores.get(primary_emotion, 50) * emotions[\"primary\"][\"intensity\"]\n",
        "    secondary_score = base_scores.get(secondary_emotion, 30) * emotions[\"secondary\"][\"intensity\"]\n",
        "\n",
        "\n",
        "    topic_weight = max(float(len(topics.get(\"main\", []))), 1.5)\n",
        "\n",
        "    overall_score = int((primary_score + secondary_score) * (topic_weight * 0.5))\n",
        "\n",
        "    breakdown_weights = {\"Delivery\": 1.5, \"Quality\": 1.2, \"Clothes\": 0.7}\n",
        "    total_weight = sum(breakdown_weights.values())\n",
        "\n",
        "    breakdown = {\n",
        "        topic: int((breakdown_weights.get(topic, 1) / total_weight) * overall_score * 1.8)\n",
        "        for topic in topics[\"main\"]\n",
        "    }\n",
        "\n",
        "    return {\"overall\": overall_score, \"breakdown\": breakdown}\n",
        "\n",
        "def analyze_feedback_dynamic():\n",
        "    text = input(\"Enter feedback text: \")\n",
        "    industry = input(\"Enter industry (ecommerce/healthcare): \").strip().lower() or \"ecommerce\"\n",
        "\n",
        "    valid_industries = [\"ecommerce\", \"healthcare\"]\n",
        "    if industry not in valid_industries:\n",
        "        print(f\"Invalid industry. Defaulting to 'ecommerce'.\")\n",
        "        industry = \"ecommerce\"\n",
        "\n",
        "    emotions = analyze_emotions(text)\n",
        "    topics = analyze_topics(text, industry)\n",
        "    adorescore = compute_adorescore(emotions, topics)\n",
        "\n",
        "    result = {\n",
        "        \"emotions\": emotions,\n",
        "        \"topics\": topics,\n",
        "        \"adorescore\": adorescore\n",
        "    }\n",
        "\n",
        "    print(\"\\nAnalysis Result:\\n\")\n",
        "    print(json.dumps(result, indent=4))\n",
        "\n",
        "analyze_feedback_dynamic()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_IaFzj-1FW9",
        "outputId": "a11f9fc5-bb7f-4c89-8af6-85d53b7f9f8c",
        "collapsed": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter feedback text: delivery is fast\n",
            "Enter industry (ecommerce/healthcare): ecoomerce\n",
            "Invalid industry. Defaulting to 'ecommerce'.\n",
            "Probabilities: {'Anger': 0.0, 'Content': 0.04, 'Disappointment': 0.0, 'Joy': 0.96}\n",
            "Primary Emotion (Before Thresholding): Joy 0.96\n",
            "\n",
            "Analysis Result:\n",
            "\n",
            "{\n",
            "    \"emotions\": {\n",
            "        \"primary\": {\n",
            "            \"emotion\": \"Joy\",\n",
            "            \"activation\": \"Medium\",\n",
            "            \"intensity\": 0.8\n",
            "        },\n",
            "        \"secondary\": {\n",
            "            \"emotion\": \"Content\",\n",
            "            \"activation\": \"Low\",\n",
            "            \"intensity\": 0.3\n",
            "        },\n",
            "        \"confidence_scores\": {\n",
            "            \"Anger\": 0.0,\n",
            "            \"Content\": 0.04,\n",
            "            \"Disappointment\": 0.0,\n",
            "            \"Joy\": 0.96\n",
            "        }\n",
            "    },\n",
            "    \"topics\": {\n",
            "        \"main\": [\n",
            "            \"Delivery\"\n",
            "        ],\n",
            "        \"subtopics\": {\n",
            "            \"Delivery\": [\n",
            "                \"fast delivery\",\n",
            "                \"slow delivery\"\n",
            "            ]\n",
            "        }\n",
            "    },\n",
            "    \"adorescore\": {\n",
            "        \"overall\": 69,\n",
            "        \"breakdown\": {\n",
            "            \"Delivery\": 54\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ruv3-OSkJQ1K",
        "outputId": "37ac735d-1daf-45c2-d5bb-74a0f07ae3cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.18.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SruX01lLAq7H",
        "outputId": "5daccf19-564b-4238-fb14-fe7f7691c141",
        "collapsed": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from langdetect import detect\n",
        "from deep_translator import GoogleTranslator\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "model = joblib.load(\"random_forest_emotion_model.pkl\")\n",
        "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
        "tfidf = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "\n",
        "theme_subtopics = {\n",
        "    \"Delivery\": [\"Fast Delivery\", \"Quick Delivery\", \"Free Delivery\", \"Good Delivery\"],\n",
        "    \"Quality\": [\"Durability\", \"Material\", \"Workmanship\"]\n",
        "}\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize VADER\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    sentiment_score = sia.polarity_scores(text)['compound']\n",
        "\n",
        "    if sentiment_score > 0.05:\n",
        "        return \"Positive\"\n",
        "    elif sentiment_score < -0.05:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "def detect_and_translate(text):\n",
        "    detected_lang = detect(text)\n",
        "    return GoogleTranslator(source=detected_lang, target=\"en\").translate(text) if detected_lang != \"en\" else text\n",
        "\n",
        "def extract_top_themes(text):\n",
        "    tfidf_matrix = tfidf.transform([text])\n",
        "    feature_names = tfidf.get_feature_names_out()\n",
        "    sorted_indices = np.argsort(tfidf_matrix.toarray()[0])[::-1]\n",
        "    top_keywords = [feature_names[i] for i in sorted_indices[:3]]\n",
        "    return top_keywords\n",
        "\n",
        "def analyze_feedback(text, industry=\"ecommerce\"):\n",
        "    emotions = analyze_emotions(text)\n",
        "    topics = analyze_topics(text, industry)\n",
        "    adorescore = compute_adorescore(emotions, topics)\n",
        "    top_themes = extract_top_themes(text)\n",
        "    sentiment = get_sentiment(text)\n",
        "    main_emotion = emotions[\"primary\"][\"emotion\"]\n",
        "\n",
        "    # Generate AI-powered response\n",
        "    ai_response = generate_response(sentiment, main_emotion)\n",
        "\n",
        "    return {\n",
        "        \"emotions\": emotions,\n",
        "        \"topics\": topics,\n",
        "        \"adorescore\": adorescore,\n",
        "        \"top_themes\": top_themes,\n",
        "        \"sentiment\": sentiment,\n",
        "        \"ai_response\": ai_response\n",
        "    }\n",
        "\n",
        "def generate_radar_chart(title):\n",
        "    categories = ['Joy', 'Sadness', 'Anger', 'Surprise', 'Fear', 'Disgust']\n",
        "    values = np.random.rand(6)\n",
        "    return go.Figure(data=[go.Scatterpolar(r=values, theta=categories, fill='toself', name=title)])\n",
        "\n",
        "def analyze_and_display(text, industry):\n",
        "    analysis = analyze_feedback(text, industry)\n",
        "    adorescore_value = analysis[\"adorescore\"][\"overall\"]\n",
        "    main_emotion = analysis[\"emotions\"][\"primary\"][\"emotion\"]\n",
        "    main_intensity = round(analysis[\"emotions\"][\"primary\"][\"intensity\"] * 100, 2)\n",
        "    top_themes = analysis[\"top_themes\"]\n",
        "    sentiment = analysis[\"sentiment\"]\n",
        "    ai_response = analysis[\"ai_response\"]\n",
        "\n",
        "    adorescore_display = f\"\"\"\n",
        "    <div style=\"text-align: center; font-size: 24px; font-weight: bold; padding: 20px; border: 2px solid #ddd; border-radius: 10px; width: 300px; margin: auto;\">\n",
        "        Adorescore <br>\n",
        "        <span style=\"font-size: 40px; color: {'green' if adorescore_value > 0 else 'red'};\">{adorescore_value}</span>\n",
        "        <br>\n",
        "        <small>Driven by</small><br>\n",
        "        <span style=\"font-size: 20px;\">{main_emotion} - {main_intensity}%</span>\n",
        "        <br><br>\n",
        "        <b>Sentiment:</b> <span style=\"color: {'green' if sentiment == 'Positive' else 'red' if sentiment == 'Negative' else 'gray'};\">{sentiment}</span>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    themes_display = f\"\"\"\n",
        "    <div style=\"text-align: center; font-size: 18px; font-weight: bold; padding: 10px; border: 1px solid #ddd; border-radius: 10px; width: 300px; margin: auto;\">\n",
        "        <b>Top Themes:</b> {\", \".join(top_themes)}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    response_display = f\"\"\"\n",
        "    <div style=\"text-align: center; font-size: 18px; padding: 10px; border: 1px solid #ddd; border-radius: 10px; width: 300px; margin: auto; background-color: #f9f9f9;\">\n",
        "        <b>Suggested Response:</b><br>\n",
        "        <i>{ai_response}</i>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    radar_high = generate_radar_chart(\"High Activation\")\n",
        "    radar_medium = generate_radar_chart(\"Medium Activation\")\n",
        "    radar_low = generate_radar_chart(\"Low Activation\")\n",
        "\n",
        "    return adorescore_display + themes_display + response_display, json.dumps(analysis, indent=4), radar_high, radar_medium, radar_low\n",
        "\n",
        "def generate_response(sentiment, emotion):\n",
        "    if sentiment == \"Positive\":\n",
        "        if emotion == \"Joy\":\n",
        "            return \"We're delighted to hear that you had a great experience! \"\n",
        "        elif emotion == \"Surprise\":\n",
        "            return \"Thank you for your feedback! Weâ€™re happy we exceeded your expectations. \"\n",
        "        else:\n",
        "            return \"We appreciate your positive feedback. Let us know how we can serve you even better!\"\n",
        "\n",
        "    elif sentiment == \"Negative\":\n",
        "        if emotion == \"Anger\":\n",
        "            return \"We're really sorry to hear that. Please reach out, and we'll fix this ASAP. \"\n",
        "        elif emotion == \"Sadness\":\n",
        "            return \"We apologize for the inconvenience. Our team is here to help resolve any issues.\"\n",
        "        else:\n",
        "            return \"We're sorry you had this experience. Let us know how we can make things right!\"\n",
        "\n",
        "    else:\n",
        "        return \"Thank you for your feedback. If you have any suggestions, weâ€™d love to hear them!\"\n",
        "\n",
        "\n",
        "def update_subtopics(selected_theme):\n",
        "    return \", \".join(theme_subtopics.get(selected_theme, [\"No subtopics available\"]))\n",
        "\n",
        "title = \"Customer Emotion Analysis System\"\n",
        "description = \"Analyze customer feedback emotions with AI-powered insights and visualization.\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(f\"### {title}\")\n",
        "    gr.Markdown(description)\n",
        "\n",
        "    with gr.Row():\n",
        "        text_input = gr.Textbox(label=\"Enter Customer Feedback\", lines=3)\n",
        "        industry_dropdown = gr.Dropdown([\"ecommerce\", \"healthcare\"], label=\"Industry\")\n",
        "\n",
        "    analyze_btn = gr.Button(\"Analyze\")\n",
        "\n",
        "    adorescore_output = gr.HTML(label=\"Adorescore & Themes\")\n",
        "    analysis_output = gr.Textbox(label=\"Analysis Results\")\n",
        "    radar_high = gr.Plot(label=\"High Activation\")\n",
        "    radar_medium = gr.Plot(label=\"Medium Activation\")\n",
        "    radar_low = gr.Plot(label=\"Low Activation\")\n",
        "\n",
        "    analyze_btn.click(\n",
        "        analyze_and_display,\n",
        "        inputs=[text_input, industry_dropdown],\n",
        "        outputs=[adorescore_output, analysis_output, radar_high, radar_medium, radar_low]\n",
        "    )\n",
        "    gr.Markdown(\"### Additional Options\")\n",
        "    with gr.Row():\n",
        "        theme_dropdown = gr.Dropdown([\"Delivery\", \"Quality\"], label=\"Themes\")\n",
        "        subtopic_display = gr.Textbox(label=\"Subtopics\", interactive=False)\n",
        "\n",
        "    theme_dropdown.change(update_subtopics, inputs=theme_dropdown, outputs=subtopic_display)\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "_vNVpoFJCIJ6",
        "outputId": "ff375183-389f-4b12-99bd-cdfe55e37191"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d376be2c4cc2a352c1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d376be2c4cc2a352c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}